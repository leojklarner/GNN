{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Graph-Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structured objects are at the heart of drug discovery and biomedical science. Be it when optimising the shape of small molecules to bind in narrow protein pockets or analysing the topology of protein-protein interaction networks to find novel and effective targets, learning from large datasets of structured objects was and is a key challenge  when trying to address these biomedical problems with computational solutions.\n",
    "\n",
    "As such, researchers have come up with a plethora of ways to convert the structural information of these objects of interest into a machine readable format. These started with domain experts hand-crafting rigid features (such as pharmacophore fingerprints) and have progressed to sophisticated usupervised machine learning methods that learn task-specific embeddings (such as variational autoencoders).\n",
    "\n",
    "Over the past years, graph machine learning has crystallised out as the pre-eminent state-of-the-art approach (https://arxiv.org/pdf/2012.05716.pdf). By blurring the distinction between feature engineering and task learning, graph-convolutional neural networks are able to operate directly on nodes and edges, making maximal use of known structural properties.\n",
    "\n",
    "As such, I decided to try to implement a graph-convolutional neural network from scratch (using only numpy). I will place particular focus on the backpropagation-driven gradient descent used to train these models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural networks will be trained on a ChEMBL-derived lipophilicity dataset that consists of the SMILES-representation and measured aqueous solubility for 4200 molecules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from abc import ABCMeta, abstractmethod\n",
    "from rdkit.Chem import AllChem, Descriptors, MolFromSmiles\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = pd.read_csv('lipophilicity.csv')\n",
    "features_smiles = data['smiles'].to_list()\n",
    "labels = data['exp'].to_numpy()\n",
    "\n",
    "# generate the first 10 descriptors specified by rdkit\n",
    "def calcDescriptors(smiles):\n",
    "    descriptors = {d[0]: d[1] for d in Descriptors.descList[:10]}\n",
    "    desc = np.zeros((len(smiles), len(descriptors)))\n",
    "    for i in range(len(smiles)):\n",
    "        mol = MolFromSmiles(smiles[i])\n",
    "        try:\n",
    "            features = [descriptors[d](mol) for d in descriptors]\n",
    "        except:\n",
    "            raise Exception('molecule {}'.format(i) + ' is not canonicalised')\n",
    "        desc[i, :] = features\n",
    "\n",
    "    return desc\n",
    "\n",
    "features_descriptors = calcDescriptors(features_smiles)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "features_descriptors = scaler.fit_transform(features_descriptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting with a Feed-Forward Neural Network\n",
    "\n",
    "Before delving into sophisticated and convoluted GNNs, it might be useful to understand and implement key concepts and algorithms on a much simpler feed-forward neural network.\n",
    "\n",
    "![A fully connected feed-forward neural net](ffnn.png)\n",
    "\n",
    "A fully-connected feed-forward neural network consists of a series of linear transformations that are interspersed by non-linear activation functions, delimiting its layers. The weights of these linear transformations are iteratively trained with gradient descent to minimise an arbitrary loss function.\n",
    "\n",
    "## Gradient Descent and Backpropagation\n",
    "\n",
    "A fully connected neural network layer is defined as\n",
    "\n",
    "$$\\mathbf{Y}=\\sigma(\\mathbf{Z})=\\sigma(\\mathbf{X}\\mathbf{W}+\\mathbf{b})$$\n",
    "\n",
    "where $\\mathbf{X}\\in \\mathbf{R}^{N\\times D}$ is the input, $\\mathbf{W}\\in \\mathbf{R}^{D\\times D'}$ and $\\mathbf{b}\\in \\mathbf{R}^{D'}$ the layer weights and biases, $\\mathbf{Y}\\in \\mathbf{R}^{N\\times D'}$ the layer ouytput and $\\sigma:\\mathbf{R}^{D'}\\to\\mathbf{R}^{D'}$ the non-linear activation function.\n",
    "\n",
    "As mentioned above, the weights in neural networks are trained trough gradient descent\n",
    "\n",
    "$$\\mathbf{W}\\gets \\mathbf{W}-\\alpha\\frac{\\partial E}{\\partial \\mathbf{W}}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate and $E$ the loss. By exploiting the sequential nature of the network, the gradients can be calculated in an efficient iterative approach known as backpropagation. For example, the gradient of the weights in the last layer are given by\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial \\mathbf{W}}= \\frac{\\partial E}{\\partial \\mathbf{Y}}\\cdot\\frac{\\partial \\mathbf{Y}}{\\partial \\mathbf{Z}}\\cdot\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{W}}$$\n",
    "\n",
    "which evaluates to\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial \\mathbf{W}}=\\mathbf{X}^T\\cdot\\frac{\\partial E}{\\partial \\mathbf{Y}}\\odot\\sigma^\\prime(\\mathbf{Z})$$\n",
    "\n",
    "where $\\odot$ denotes element-wise multiplication and ${\\partial E}/{\\partial \\mathbf{Y}}$ depends on the chosen loss function. Similarly, the derivation by $\\mathbf{b}$ and $\\mathbf{X}$ is given by\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial \\mathbf{b}} = \\frac{\\partial E}{\\partial \\mathbf{Y}}\\cdot\\frac{\\partial \\mathbf{Y}}{\\partial \\mathbf{Z}}\\cdot\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{b}} = \\frac{\\partial E}{\\partial \\mathbf{Y}}\\odot\\sigma^\\prime(\\mathbf{Z})\\cdot 1$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\\frac{\\partial E}{\\partial \\mathbf{X}} = \\frac{\\partial E}{\\partial \\mathbf{Y}}\\cdot\\frac{\\partial \\mathbf{Y}}{\\partial \\mathbf{Z}}\\cdot\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{X}} = \\frac{\\partial E}{\\partial \\mathbf{Y}}\\odot\\sigma^\\prime(\\mathbf{Z})\\cdot \\mathbf{W}^T$$\n",
    "\n",
    "where it is already apparent that parts of the expression can be pre-computed and re-used.\n",
    "\n",
    "The application of the product rule can then be sequentially continued throughout the network. If $\\mathbf{X}$ is, for instance, the output of a previous layer parametrised by the weight matrix $\\mathbf{W}^\\prime$, the respective derivative is given by \n",
    "\n",
    "$$\\frac{\\partial E}{\\partial \\mathbf{W}^\\prime} = \\frac{\\partial E}{\\partial \\mathbf{Y}}\\cdot\\frac{\\partial \\mathbf{Y}}{\\partial \\mathbf{Z}}\\cdot\\frac{\\partial \\mathbf{Z}}{\\partial \\mathbf{X}}\\cdot\\frac{\\partial \\mathbf{X}}{\\partial \\mathbf{W}^\\prime}$$\n",
    "\n",
    "where large parts of the calculation can, again, be pre-computed and re-used. This is continued until the last layer, and the respective weights are updated along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "We start by implementing a base layer class from which more complex architectures will be derived."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(metaclass=ABCMeta):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.input = None\n",
    "        self.output = None\n",
    "        \n",
    "    @abstractmethod\n",
    "    def forward(self, input):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @abstractmethod\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        raise NotImplementedError        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the activation function does not contain any trainable parameters, the linear transformation and activation layers are typically implemented separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(Layer):\n",
    "\n",
    "    def __init__(self, input_dimensions, output_dimensions):\n",
    "        super(FullyConnectedLayer, self).__init__()\n",
    "        self.weights = np.random.randn(input_dimensions, output_dimensions)\n",
    "        self.bias = np.zeros((1, output_dimensions))\n",
    "        self.xavier_initialisation()\n",
    "\n",
    "    def xavier_initialisation(self):\n",
    "        \"\"\"\n",
    "        Xavier initialisation scales the parameters so that their standard\n",
    "        deviation remains roughly equal across layers.\n",
    "        \"\"\"\n",
    "        self.weights = self.weights * np.sqrt(6 / np.sum(self.weights.shape))\n",
    "\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        A forward pass applies the affine transformation defined by the\n",
    "        weights and bias parameters to the input points.\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        self.output = np.dot(self.input, self.weights) + self.bias\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, output_error, learning_rate):\n",
    "        \"\"\"\n",
    "        While the backward pass accepts the derived error from the succeeding\n",
    "        layer, calculated the gradients for its parameters and passes the \n",
    "        gradient with respect to the inputs to the preceding layer.\n",
    "        \"\"\"\n",
    "        input_error = np.dot(output_error, self.weights.T)\n",
    "        weights_error = np.dot(self.input.T, output_error)\n",
    "        bias_error = output_error\n",
    "        \n",
    "        self.weights -= learning_rate*weights_error\n",
    "        self.bias -= learning_rate*bias_error\n",
    "        \n",
    "        return input_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining commonly used non-linearities\n",
    "\n",
    "def lrelu(x, alpha=0.2):\n",
    "    return np.where(x<0, alpha*x, x)\n",
    "    \n",
    "def lrelu_deriv(x, alpha=0.2):\n",
    "    return np.where(x<0, alpha, 1)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x);\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1-np.tanh(x)**2;\n",
    "\n",
    "\n",
    "class ActivationLayer(Layer):\n",
    "\n",
    "    def __init__(self, activation_forward, activation_backward):\n",
    "        super(ActivationLayer, self).__init__()\n",
    "        self.activation_forward = activation_forward\n",
    "        self.activation_backward = activation_backward\n",
    "\n",
    "    # returns the activated input\n",
    "    def forward(self, input):\n",
    "        \"\"\"\n",
    "        The forward pass simply applies the non-linear transformation.\n",
    "        \"\"\"\n",
    "        self.input = input\n",
    "        self.output = self.activation_forward(self.input)\n",
    "        return self.output\n",
    "\n",
    "    # Returns input_error=dE/dX for a given output_error=dE/dY.\n",
    "    # learning_rate is not used because there is no \"learnable\" parameters.\n",
    "    def backward(self, output_error, learning_rate):\n",
    "        \"\"\"\n",
    "        While a backward pass multiplies the derivative at the input points\n",
    "        in an element-wise fashion.\n",
    "        \"\"\"\n",
    "        return self.activation_backward(self.input) * output_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create the model class that accepts any number of these layers in a specific architecture and fits the model through repeated backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model:\n",
    "\n",
    "    def __init__(self, loss_forward, loss_backward):\n",
    "        self.layers = []\n",
    "        self.loss_forward = loss_forward\n",
    "        self.loss_backward = loss_backward\n",
    "\n",
    "    def add_layer(self, layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def predict(self, input):\n",
    "        \"\"\"\n",
    "        Iterates through the input samples and performs a forward pass through \n",
    "        all layers, returning the respective results.\n",
    "        \"\"\"\n",
    "        \n",
    "        result = np.zeros(input.shape[0])\n",
    "\n",
    "        for i in range(input.shape[0]):\n",
    "            output = input[i]\n",
    "            for layer in self.layers:\n",
    "                output = layer.forward(output)\n",
    "            result[i] = (output)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def fit(self, x_train, y_train, epochs, learning_rate):\n",
    "        \"\"\"\n",
    "        Calculated the gradients through backpropagation and performs\n",
    "        gradient descent for the given number of epochs with the \n",
    "        specified learning rate.\n",
    "        \"\"\"\n",
    "\n",
    "        err_log=[]\n",
    "        \n",
    "        # in each epoch\n",
    "        for i in range(epochs):\n",
    "            err = 0\n",
    "            \n",
    "            # go through each sample\n",
    "            for j in range(x_train.shape[0]):\n",
    "                \n",
    "                # perform a forward pass\n",
    "                output = x_train[j].reshape(1,-1)\n",
    "                for layer in self.layers:\n",
    "                    output = layer.forward(output)\n",
    "\n",
    "                # and calculate the loss\n",
    "                err += self.loss_forward(y_train[j], output)\n",
    "\n",
    "                # perform a backward pass\n",
    "                error = self.loss_backward(y_train[j], output)\n",
    "\n",
    "                # and modify gradients in each layer\n",
    "                for layer in reversed(self.layers):\n",
    "                    error = layer.backward(error, learning_rate)\n",
    "\n",
    "                    \n",
    "            # calculate the average error over all samples and return it for analysis\n",
    "            err /= x_train.shape[0]\n",
    "            err_log.append(err)\n",
    "                \n",
    "        return err_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now specify the loss function we want our model to minimise and add 3 layers with varying numbers of neurons, using the $\\tanh$ function as a non-linearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 100/100   error=0.966568\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def mse_forward(y_true, y_pred):\n",
    "    return np.mean(np.power(y_true-y_pred, 2))\n",
    "\n",
    "def mse_backward(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size\n",
    "\n",
    "net = model(mse_forward, mse_backward)\n",
    "net.add_layer(FullyConnectedLayer(10,20))\n",
    "net.add_layer(ActivationLayer(tanh, tanh_prime))\n",
    "net.add_layer(FullyConnectedLayer(20,10))\n",
    "net.add_layer(ActivationLayer(tanh, tanh_prime))\n",
    "net.add_layer(FullyConnectedLayer(10,5))\n",
    "net.add_layer(ActivationLayer(tanh, tanh_prime))\n",
    "net.add_layer(FullyConnectedLayer(5,1))\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "log = net.fit(features_descriptors, labels, epochs=num_epochs, learning_rate=0.001)\n",
    "\n",
    "plt.plot(list(range(num_epochs)), log, label='training loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training it for 100 epochs on the continuous rdkit features we generated above, we can see that the implementation was successful and our model minimises the loss function and fits the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progressing to a Graph-Convolutional Network\n",
    "\n",
    "Istead of operating on numeric feature vectors, as the feed-forward neural net above, graph-convolutional networks operate directly on graphs. I will focus on a widely-used architecture called Graph Attention Networks (GATs, https://arxiv.org/abs/1710.10903), although it should be said that there is a large number of different GNN architectures.\n",
    "\n",
    "A GAT consists of layers that accept a set of feature vectors for each node and return a set of altered features, potentially adding or reducing dimensions.\n",
    "\n",
    "$$ \\left(\\vec{h}_{1}, \\vec{h}_{2}, \\ldots, \\vec{h}_{n}\\right) \\in \\mathbf{R}^{n\\times f} \\to\\left(\\vec{h}_{1}^{\\prime}, \\vec{h}_{2}^{\\prime}, \\ldots, \\vec{h}_{n}^{\\prime}\\right)\\in \\mathbf{R}^{n\\times f^\\prime}$$ \n",
    "\n",
    "The general architecture of a graph convolutional layer is given by \n",
    "\n",
    "$$\n",
    "\\vec{h}_{i}^{\\prime}=\\sigma\\left(\\sum_{j \\in \\mathcal{N}_{i}} \\alpha_{i j} \\vec{g}_{j}\\right)\n",
    "$$\n",
    "\n",
    "where $\\vec{g}_{i}=\\mathbf{W} \\vec{h}_{i}$ and $\\sigma$ is a non-linear activation function. The GAT approach differs from other established methods by not specifying the coefficients $\\alpha_{i j}$ beforehand, but calculating them through self-attention.\n",
    "\n",
    "$$\n",
    "\\alpha_{i j}=\\frac{\\exp \\left(e_{i j}\\right)}{\\sum_{k \\in \\mathcal{N}_{i}} \\exp \\left(e_{i k}\\right)}=\\frac{\\exp (a(\\vec{h}_{i}, \\vec{h}_{j}))}{\\sum_{k \\in \\mathcal{N}_{i}} \\exp (a(\\vec{h}_{i}, \\vec{h}_{k}))}\n",
    "$$\n",
    "\n",
    "where $a(\\cdot,\\cdot)$ is a $2f^\\prime\\times 1$ single-layer feed-forward neural network trained on the concatenated feature vectors \n",
    "\n",
    "$$a(\\vec{h}_{i}, \\vec{h}_{j})=\\sigma\\left(\\vec{\\mathbf{a}}^{T}\\left[\\mathbf{W} \\vec{h}_{i} \\| \\mathbf{W} \\vec{h}_{j}\\right]\\right)$$\n",
    "\n",
    "The attention mechanism allows the network to implicitly learn which substructures are more important for specific tasks and weigh them appropriately. The relative complexity of this model allows it to achieve state-of-the-art performance, but makes the implementation from scratch all the more difficult, as trying to implement the backpropagation with respect to the weight vector $\\vec{\\mathbf{a}}$ shows \n",
    "\n",
    "$$\\frac{\\partial E}{\\partial \\vec{\\mathbf{a}}}=\\frac{\\partial E}{\\partial \\vec{h}_{i}}\\frac{\\partial  \\vec{h}_{i}}{\\partial \\vec{\\mathbf{a}}}=\\frac{\\partial E}{\\partial \\vec{h}_{i}}\\sum_{j \\in \\mathcal{N}_{i}} \\frac{\\partial \\alpha_{i j}}{\\partial \\vec{\\mathbf{a}}} \\vec{g}_{j}$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\alpha_{i j}}{\\partial \\vec{\\mathbf{a}}}=\\frac{\\exp \\left(e_{i j}\\right)}{\\sum_{k \\in \\mathcal{N}_{i}} \\exp \\left(e_{i k}\\right)}\\left(\\frac{\\partial e_{i j}}{\\partial \\vec{\\mathbf{a}}}-\\frac{\\sum_k\\frac{\\partial e_{i k}}{\\partial \\vec{\\mathbf{a}}}\\exp(e_{ik})}{\\sum_{k \\in \\mathcal{N}_{i}} \\exp \\left(e_{i k}\\right)}\\right)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial e_{i j}}{\\partial \\vec{\\mathbf{a}}}=\\sigma^\\prime\\left(\\vec{\\mathbf{a}}^{T}\\left[\\mathbf{W} \\vec{h}_{i} \\| \\mathbf{W} \\vec{h}_{j}\\right]\\right)\\odot \\left[\\mathbf{W} \\vec{h}_{i} \\| \\mathbf{W} \\vec{h}_{j}\\right]\n",
    "$$\n",
    "\n",
    "While implementing a forward pass was relatively straight-forward, calculating and implementing the convoluted derivatives needed for the backward passes proved too complex and time-consuming for the limited duration of this project.\n",
    "\n",
    "I definitely feel that I have gained a much better understanding of the backpropagation algorithm, but at the same time would probably use an existing machine learning framework (PyTorch, TensorFLow, ...) with built-in automated backwards passes, if I were to continue the implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphAttentionLayer(Layer):\n",
    "\n",
    "    # input is list of node features\n",
    "\n",
    "    # where input and output dims are the number of node features\n",
    "    def __init__(self, A, input_dims, output_dims):\n",
    "        \"\"\"\n",
    "        Initialising the GAT layer, where A is the graph adjacency matrix and the weights\n",
    "        are initialised according to the Xavier method.\n",
    "        \"\"\"\n",
    "        super(GraphAttentionLayer, self).__init__()\n",
    "        self.A = A\n",
    "        self.g = None\n",
    "        self.weights = np.random.randn(input_dims, output_dims) * np.sqrt(6 / (input_dims + output_dims))\n",
    "        self.attention_weights = np.random.randn(2 * output_dims, 1) * np.sqrt(6 / (2 * output_dims + 1))\n",
    "\n",
    "    def _neighbours(self, node):\n",
    "        \"\"\"\n",
    "        Returning a list of the neighbours of a specific node.\n",
    "        \"\"\"\n",
    "        return np.nonzero(self.A[node, :])[0]\n",
    "\n",
    "    def _calc_attention(self, node):\n",
    "        \"\"\"\n",
    "        calculating the normalised attention coefficients\n",
    "        \"\"\"\n",
    "        # calculating the unnormalised attention coefficients\n",
    "        attention_coefs = np.zeros(self.input.shape[0])\n",
    "        for n in self._neighbours(node):\n",
    "            attention_features = np.hstack((self.g[node, :], self.g[n, :]))\n",
    "            a = lrelu(np.dot(self.attention_weights.T, attention_features))\n",
    "            attention_coefs[n] = a\n",
    "\n",
    "        # normalising attention coefficients\n",
    "        attention_coefs = np.where(attention_coefs != 0, np.exp(attention_coefs), 0)\n",
    "        normalised_coefs = attention_coefs / np.sum(attention_coefs)\n",
    "\n",
    "        return np.array(normalised_coefs)\n",
    "\n",
    "    def forward_propagation(self, input):\n",
    "        \"\"\"\n",
    "        Performing a forward pass through the network and calculating the \n",
    "        updated feature vectors\n",
    "        \"\"\"\n",
    "\n",
    "        self.input = input\n",
    "        self.g = self.input @ self.weights\n",
    "\n",
    "        res = []\n",
    "\n",
    "        for node in range(self.input.shape[0]):\n",
    "            # get coefficients\n",
    "            coefs = self._calc_attention(node)\n",
    "            neighs = self._neighbours(node)\n",
    "            summed = np.zeros(self.g.shape[1])\n",
    "            for n in neighs:\n",
    "                summed += coefs[n]*self.g[n, :]\n",
    "            res.append(summed)\n",
    "\n",
    "        return np.vstack(res)\n",
    "\n",
    "    def backward_propagation(self, output_error, learning_rate):\n",
    "        \"\"\"\n",
    "        Performing a backward pass through the network and updating its parameters,\n",
    "        currently only implemented for the weights of the feed-forward network\n",
    "        used to calculate the attention coefficients.\n",
    "        \"\"\"\n",
    "\n",
    "        def e(i, j):\n",
    "            return lrelu(np.dot(self.attention_weights.T, np.hstack((self.g[i, :], self.g[j, :]))))\n",
    "\n",
    "        def de_da(i, j):\n",
    "            inner = np.hstack((self.g[i, :], self.g[j, :]))\n",
    "            outer = lrelu_prime(np.dot(self.attention_weights.T, inner))\n",
    "            return inner * outer\n",
    "\n",
    "        def dalpha_da(i, j):\n",
    "            summed = 0\n",
    "            summed_dif = 0\n",
    "            for k in self._neighbours(i):\n",
    "                summed += np.exp(e(i, k))\n",
    "                summed_dif += de_da(i, k) * np.exp(e(i, k))\n",
    "\n",
    "            return np.exp(e(i, j)) / summed * (de_da(i, j) - summed_dif / summed)\n",
    "\n",
    "        nodes, dims = self.g.shape\n",
    "        attention_error = np.zeros((nodes, 2*dims))\n",
    "\n",
    "        for node in range(nodes):\n",
    "            for n in self._neighbours(node):\n",
    "                attention_error[node,:] += dalpha_da(node, n)\n",
    "\n",
    "        attention_error = np.mean(attention_error, axis=0)\n",
    "\n",
    "        self.attention_weights -= learning_rate*attention_error.reshape(-1,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
